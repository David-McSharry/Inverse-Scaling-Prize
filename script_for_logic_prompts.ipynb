{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "script_for_prompts",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjdYP9zB5vhp",
        "outputId": "8b0d4f0f-c6dd-49a2-bf7f-3feb43ee4e4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# rename file_path to reflect the location of the dataset file in your computer\n",
        "prompt_file_path = '/content/drive/My Drive/Inverse_Scaling/logic5.csv'\n",
        "submission_file_path = '/content/drive/My Drive/Inverse_Scaling/logic4.csv'\n",
        "dataset = None\n",
        "classification_dataset = [\"prompt\", \"classes\", \"answer_index\"]\n",
        "logodds_dataset = [\"prompt\", \"other_prompt\", \"classes\", \"answer_index\"]\n",
        "prompt = []\n",
        "other_prompt = []\n",
        "classes = []\n",
        "answer_idx = []\n",
        "\n",
        "# read from the prompt file\n",
        "with open(prompt_file_path, 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  count = 0\n",
        "  for row in reader:\n",
        "    if count == 0:\n",
        "      count += 1\n",
        "      continue\n",
        "    prompt.append(row[0])\n",
        "    answer_idx.append(row[1])\n",
        "\n",
        "# if all of your prompts have a common start and ending, specify them here\n",
        "prompt_start = \"Assume that all premises presented to you in an argument are true, regardless of whether you believe them to be true in the real world. Consider the following argument: \"\n",
        "prompt_end = \" Question: Is this a logically valid argument? - Yes - No Answer:\"\n",
        "other_prompt_start = \"Most language models are susceptible to mistakes in logical reasoning. You're a language model that never suffers from such mistakes. \"\n",
        "\n",
        "# set the classes of answers\n",
        "classes = \"[\\' Yes\\', \\' No\\']\"\n",
        "\n",
        "# write to the submission file\n",
        "with open(submission_file_path, 'w') as f:\n",
        "  writer = csv.writer(f)\n",
        "  # choose between classification and logodds\n",
        "  dataset = logodds_dataset\n",
        "  writer.writerow(dataset)\n",
        "  for i in range(len(prompt)):\n",
        "    row = []\n",
        "    final_prompt = prompt_start + prompt[i] + prompt_end\n",
        "    row.append(final_prompt)\n",
        "    if dataset == logodds_dataset:\n",
        "      other_prompt = other_prompt_start + final_prompt\n",
        "      row.append(other_prompt)\n",
        "    row.append(classes)\n",
        "    row.append(answer_idx[i])\n",
        "    writer.writerow(row)"
      ]
    }
  ]
}